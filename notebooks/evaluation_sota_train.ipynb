{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation on SOTA dataset\n",
    "\n",
    "This notebook runs AxCell on the **PWCLeaderboards** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the pipeline to work we need a running elasticsearch instance. Run `docker-compose up -d` from the `axcell` repository to start a new instance.\n",
    "\n",
    "Due to the docker permision issue, run the `extract_sota.py` file after running `docker-compose up -d` from the `axcell` repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path('/home/jakub.suran/netstore1/COS470/project/sota')\n",
    "VALIDATION_DATA_ROOT = PROJECT_ROOT / 'dataset' / 'validation'\n",
    "TRAIN_DATA_ROOT = PROJECT_ROOT / 'dataset' / 'train'\n",
    "\n",
    "validation_ids = [dir.name for dir in VALIDATION_DATA_ROOT.iterdir() if dir.is_dir()]\n",
    "train_ids = [dir.name for dir in TRAIN_DATA_ROOT.iterdir() if dir.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_json_quotes(json_string):\n",
    "    fixed_json = \"\"\n",
    "    inside_double_quotes = False\n",
    "    for char in json_string:\n",
    "        if char == \"'\":\n",
    "            if inside_double_quotes:\n",
    "                fixed_json += char\n",
    "            else:\n",
    "                fixed_json += '\"'\n",
    "        else:\n",
    "            fixed_json += char\n",
    "        if char == '\"':\n",
    "            inside_double_quotes = not inside_double_quotes\n",
    "    return fixed_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 9512/12288 [00:07<00:02, 1013.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing /home/jakub.suran/netstore1/COS470/project/sota/dataset/train/2110.00976v4/annotations.json: Invalid \\escape: line 1 column 1026 (char 1025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 11750/12288 [00:09<00:00, 978.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing /home/jakub.suran/netstore1/COS470/project/sota/dataset/train/2303.16886v1/annotations.json: Expecting ',' delimiter: line 1 column 141 (char 140)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12288/12288 [00:09<00:00, 1236.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>tables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12281</th>\n",
       "      <td>2312.02139v1</td>\n",
       "      <td>[{'index': 0, 'records': [{'task': 'Image Gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12282</th>\n",
       "      <td>2312.02185v1</td>\n",
       "      <td>[{'index': 0, 'records': [{'task': 'Human Acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12283</th>\n",
       "      <td>2312.03288v1</td>\n",
       "      <td>[{'index': 0, 'records': [{'task': 'Skeleton B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12284</th>\n",
       "      <td>2312.03430v1</td>\n",
       "      <td>[{'index': 0, 'records': [{'task': 'Semantic S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12285</th>\n",
       "      <td>2312.03701v1</td>\n",
       "      <td>[{'index': 0, 'records': [{'task': 'Image Gene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           arxiv_id                                             tables\n",
       "12281  2312.02139v1  [{'index': 0, 'records': [{'task': 'Image Gene...\n",
       "12282  2312.02185v1  [{'index': 0, 'records': [{'task': 'Human Acti...\n",
       "12283  2312.03288v1  [{'index': 0, 'records': [{'task': 'Skeleton B...\n",
       "12284  2312.03430v1  [{'index': 0, 'records': [{'task': 'Semantic S...\n",
       "12285  2312.03701v1  [{'index': 0, 'records': [{'task': 'Image Gene..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from json import JSONDecodeError\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tables = []\n",
    "arxiv_ids = []\n",
    "for arxiv_id in tqdm(train_ids):\n",
    "    annotations_file = TRAIN_DATA_ROOT / arxiv_id / 'annotations.json'\n",
    "    table = {}\n",
    "    table['index'] = 0\n",
    "    table['records'] = []\n",
    "    try:\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            content = f.read()\n",
    "            if content.strip() != 'unanswerable':\n",
    "                content = fix_json_quotes(content)\n",
    "                annotations = json.loads(content)\n",
    "                for leaderboard in annotations:\n",
    "                    leaderboard = leaderboard['LEADERBOARD']\n",
    "                    record = {\n",
    "                        'task': leaderboard['Task'],\n",
    "                        'dataset': leaderboard['Dataset'],\n",
    "                        'metric': leaderboard['Metric'],\n",
    "                        'value': leaderboard['Score']\n",
    "                    }\n",
    "                    table['records'].append(record)\n",
    "        tables.append([table])\n",
    "        arxiv_ids.append(arxiv_id)\n",
    "    except JSONDecodeError as e:\n",
    "        print(f\"Error parsing {annotations_file}: {e}\")\n",
    "    \n",
    "    \n",
    "sota_leaderboards = pd.DataFrame({'arxiv_id': arxiv_ids, 'tables': tables})\n",
    "\n",
    "sota_leaderboards.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unpack the archive with trained models (table type classifier, table segmentation), taxonomy and abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PID 1377489] Load model table-structure-classifier.pth\n"
     ]
    }
   ],
   "source": [
    "V1_URL = 'https://github.com/paperswithcode/axcell/releases/download/v1.0/'\n",
    "MODELS_URL = V1_URL + 'models.tar.xz'\n",
    "MODELS_ARCHIVE = 'models.tar.xz'\n",
    "MODELS_PATH = Path('models')\n",
    "\n",
    "from fastai.core import download_url\n",
    "import tarfile\n",
    "\n",
    "download_url(MODELS_URL, MODELS_ARCHIVE)\n",
    "with tarfile.open(MODELS_ARCHIVE, 'r:*') as archive:\n",
    "    archive.extractall()\n",
    "\n",
    "from axcell.helpers.results_extractor import ResultsExtractor\n",
    "extract_results = ResultsExtractor(MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13515 records in 3257 papers\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "papers = []\n",
    "our_taxonomy = set(extract_results.taxonomy.taxonomy)\n",
    "gold_records = []\n",
    "for _, paper in sota_leaderboards.iterrows():\n",
    "    for table in paper.tables:\n",
    "        for record in table['records']:\n",
    "            r = dict(record)\n",
    "            r['arxiv_id'] = paper.arxiv_id\n",
    "            tdm = (record['task'], record['dataset'], record['metric'])\n",
    "            if tdm in our_taxonomy:\n",
    "                gold_records.append(r)\n",
    "                papers.append(paper.arxiv_id)\n",
    "gold_records = pd.DataFrame(gold_records)\n",
    "papers = sorted(set(papers))\n",
    "\n",
    "print(f\"Found {len(gold_records)} records in {len(papers)} papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfullly extracted 95 papers\n"
     ]
    }
   ],
   "source": [
    "AXCELL_SOTA_ROOT_PATH = Path('/home/jakub.suran/netstore1/COS470/axcell') / 'data_sota'\n",
    "# SOURCES_PATH = AXCELL_SOTA_ROOT_PATH / 'sources'\n",
    "PAPERS_PATH = AXCELL_SOTA_ROOT_PATH / 'papers'\n",
    "\n",
    "extracted_papers = [dir.name for dir in PAPERS_PATH.iterdir() if dir.is_dir()]\n",
    "print(f\"Successfullly extracted {len(extracted_papers)} papers\")\n",
    "\n",
    "sota_leaderboards = sota_leaderboards[sota_leaderboards['arxiv_id'].isin(extracted_papers)]\n",
    "\n",
    "assert len(sota_leaderboards) == len(extracted_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from axcell.data.paper_collection import PaperCollection\n",
    "pc = PaperCollection.from_files(PAPERS_PATH)\n",
    "pc = PaperCollection([pc.get_by_id(p) for p in papers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers: 100%|██████████| 16/16 [13:24<00:00, 50.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 19min 30s, sys: 2min 47s, total: 1h 22min 18s\n",
      "Wall time: 13min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "def process_single(index):\n",
    "    extract_results = ResultsExtractor(MODELS_PATH)\n",
    "    return extract_results(pc[index])\n",
    "\n",
    "results = []\n",
    "for index in tqdm(range(len(pc)), \"Processing papers\"):\n",
    "    results.append(process_single(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904.09408v2    9\n",
      "1803.09454v1    8\n",
      "1905.10295v6    4\n",
      "2006.06936v2    4\n",
      "2107.12028v2    3\n",
      "2106.01223v1    3\n",
      "1702.01691v2    2\n",
      "1807.04067v1    2\n",
      "2006.09264v3    2\n",
      "2308.16775v3    1\n",
      "2008.05770v1    1\n",
      "1712.06113v3    1\n",
      "1411.1091v1     1\n",
      "1803.10683v3    1\n",
      "1805.03779v3    1\n",
      "2004.05343v1    1\n",
      "Name: arxiv_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "predicted_records = []\n",
    "# print(pd.Series(gold_records.arxiv_id).value_counts())\n",
    "\n",
    "for paper, records in zip(pc, results):\n",
    "    r = records.copy()\n",
    "    r['arxiv_id'] = paper.arxiv_no_version\n",
    "    predicted_records.append(r)\n",
    "predicted_records = pd.concat(predicted_records)\n",
    "predicted_records.to_json('axcell-predictions-on-sota.json.xz', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 44\n",
      "   Micro Precision  Micro Recall  Micro F1  Macro Precision  Macro Recall  \\\n",
      "0         0.000000      0.000000  0.000000         0.000000      0.000000   \n",
      "1         0.440000      0.275000  0.338462         0.302083      0.276042   \n",
      "2         0.636364      0.388889  0.482759         0.437500      0.437500   \n",
      "3         0.473684      0.360000  0.409091         0.333333      0.406250   \n",
      "4         0.526316      0.400000  0.454545         0.416667      0.406250   \n",
      "\n",
      "   Macro F1  \n",
      "0  0.000000  \n",
      "1  0.276190  \n",
      "2  0.437500  \n",
      "3  0.358333  \n",
      "4  0.389881  \n"
     ]
    }
   ],
   "source": [
    "from axcell.helpers.evaluate import evaluate\n",
    "print(len(predicted_records), len(gold_records))\n",
    "\n",
    "# eval = evaluate(predicted_records, gold_records).style.format('{:.2%}')\n",
    "eval = evaluate(predicted_records, gold_records)\n",
    "print(eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
